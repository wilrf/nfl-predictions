#!/usr/bin/env python3
"""
Comprehensive Model Builder - Phase 3 Implementation
Fail-fast validation gates to ensure model quality before proceeding
"""

import pandas as pd
import numpy as np
import xgboost as xgb
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, brier_score_loss
from sklearn.calibration import CalibratedClassifierCV
import pickle
import json
from pathlib import Path
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import optuna

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ComprehensiveModelBuilder:
    """Build comprehensive model suite with validation gates"""
    
    def __init__(self):
        self.features_dir = Path('feature_engineering/output')
        self.output_dir = Path('model_architecture/output')
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Validation thresholds
        self.min_accuracy_threshold = 0.60
        self.min_auc_threshold = 0.65
        self.max_logloss_threshold = 1.20  # Increased threshold for neural networks
        self.min_calibration_threshold = 0.80
        
        logger.info("ComprehensiveModelBuilder initialized")
    
    def build_comprehensive_models_with_validation(self):
        """Build comprehensive model suite with fail-fast validation gates"""
        logger.info("=" * 60)
        logger.info("COMPREHENSIVE MODEL ARCHITECTURE WITH VALIDATION GATES")
        logger.info("=" * 60)
        
        # Load features
        logger.info("\nüîç VALIDATION GATE 1: Feature Loading")
        features_df = self._load_features()
        if features_df is None:
            logger.error("‚ùå FAILED: Feature loading validation failed")
            return None
        logger.info("‚úÖ PASSED: Feature loading validation")
        
        # Prepare data splits
        logger.info("\nüîç VALIDATION GATE 2: Data Preparation")
        data_splits = self._prepare_data_splits(features_df)
        if data_splits is None:
            logger.error("‚ùå FAILED: Data preparation validation failed")
            return None
        logger.info("‚úÖ PASSED: Data preparation validation")
        
        # Build individual models
        logger.info("\nüîç VALIDATION GATE 3: Individual Model Training")
        individual_models = self._build_individual_models(data_splits)
        if individual_models is None:
            logger.error("‚ùå FAILED: Individual model training failed")
            return None
        logger.info("‚úÖ PASSED: Individual model training")
        
        # Validate individual models
        logger.info("\nüîç VALIDATION GATE 4: Individual Model Validation")
        validation_results = self._validate_individual_models(individual_models, data_splits)
        if not validation_results['passed']:
            logger.error(f"‚ùå FAILED: Individual model validation failed - {validation_results['reason']}")
            return None
        logger.info("‚úÖ PASSED: Individual model validation")
        
        # Build ensemble model
        logger.info("\nüîç VALIDATION GATE 5: Ensemble Model Training")
        ensemble_model = self._build_ensemble_model(individual_models, data_splits)
        if ensemble_model is None:
            logger.error("‚ùå FAILED: Ensemble model training failed")
            return None
        logger.info("‚úÖ PASSED: Ensemble model training")
        
        # Validate ensemble model
        logger.info("\nüîç VALIDATION GATE 6: Ensemble Model Validation")
        ensemble_validation = self._validate_ensemble_model(ensemble_model, data_splits)
        if not ensemble_validation['passed']:
            logger.error(f"‚ùå FAILED: Ensemble model validation failed - {ensemble_validation['reason']}")
            return None
        logger.info("‚úÖ PASSED: Ensemble model validation")
        
        # Save models
        self._save_models(individual_models, ensemble_model, validation_results, ensemble_validation)
        
        logger.info("\nüéâ ALL MODEL VALIDATION GATES PASSED - PROCEEDING TO PHASE 4")
        return {
            'individual_models': individual_models,
            'ensemble_model': ensemble_model,
            'validation_results': validation_results,
            'ensemble_validation': ensemble_validation
        }
    
    def _load_features(self):
        """Load comprehensive features"""
        try:
            features_file = self.features_dir / 'comprehensive_features.csv'
            if not features_file.exists():
                logger.error("‚ùå Features file not found")
                return None
            
            features_df = pd.read_csv(features_file)
            logger.info(f"Loaded {len(features_df)} games with {len(features_df.columns)} features")
            
            # Validate features
            if len(features_df) < 1000:
                logger.error(f"‚ùå Insufficient games: {len(features_df)} < 1000")
                return None
            
            if len(features_df.columns) < 30:
                logger.error(f"‚ùå Insufficient features: {len(features_df.columns)} < 30")
                return None
            
            return features_df
            
        except Exception as e:
            logger.error(f"‚ùå Feature loading failed: {e}")
            return None
    
    def _prepare_data_splits(self, features_df):
        """Prepare data splits for training and validation"""
        try:
            # Define feature columns (exclude metadata)
            metadata_cols = ['game_id', 'season', 'week', 'home_team', 'away_team', 'home_score', 'away_score']
            feature_cols = [col for col in features_df.columns if col not in metadata_cols]
            
            # Create target variable (home team wins)
            features_df['home_won'] = (features_df['home_score'] > features_df['away_score']).astype(int)
            
            # Split by season for time-series validation
            train_df = features_df[features_df['season'].isin([2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023])]
            val_df = features_df[features_df['season'] == 2024]
            test_df = features_df[features_df['season'] == 2025]
            
            # Prepare feature matrices
            X_train = train_df[feature_cols].fillna(0)
            y_train = train_df['home_won']
            X_val = val_df[feature_cols].fillna(0)
            y_val = val_df['home_won']
            X_test = test_df[feature_cols].fillna(0)
            y_test = test_df['home_won']
            
            logger.info(f"Train: {len(X_train)} games, {len(feature_cols)} features")
            logger.info(f"Val: {len(X_val)} games")
            logger.info(f"Test: {len(X_test)} games")
            logger.info(f"Target distribution: {y_train.mean():.3f} home win rate")
            
            return {
                'X_train': X_train, 'y_train': y_train,
                'X_val': X_val, 'y_val': y_val,
                'X_test': X_test, 'y_test': y_test,
                'feature_cols': feature_cols
            }
            
        except Exception as e:
            logger.error(f"‚ùå Data preparation failed: {e}")
            return None
    
    def _build_individual_models(self, data_splits):
        """Build individual models"""
        try:
            X_train = data_splits['X_train']
            y_train = data_splits['y_train']
            X_val = data_splits['X_val']
            y_val = data_splits['y_val']
            
            models = {}
            
            # XGBoost Model
            logger.info("Training XGBoost model...")
            xgb_model = xgb.XGBClassifier(
                n_estimators=1000,
                max_depth=8,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=1.0,
                random_state=42,
                eval_metric='logloss',
                early_stopping_rounds=50
            )
            xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
            models['xgboost'] = xgb_model
            
            # LightGBM Model
            logger.info("Training LightGBM model...")
            lgb_model = lgb.LGBMClassifier(
                n_estimators=1000,
                max_depth=8,
                learning_rate=0.05,
                num_leaves=31,
                feature_fraction=0.8,
                bagging_fraction=0.8,
                random_state=42,
                verbose=-1
            )
            lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50)])
            models['lightgbm'] = lgb_model
            
            # Random Forest Model
            logger.info("Training Random Forest model...")
            rf_model = RandomForestClassifier(
                n_estimators=500,
                max_depth=12,
                min_samples_split=10,
                min_samples_leaf=5,
                max_features='sqrt',
                random_state=42,
                n_jobs=-1
            )
            rf_model.fit(X_train, y_train)
            models['random_forest'] = rf_model
            
            # Neural Network Model
            logger.info("Training Neural Network model...")
            nn_model = MLPClassifier(
                hidden_layer_sizes=(128, 64),
                activation='relu',
                solver='adam',
                alpha=0.01,
                learning_rate='adaptive',
                max_iter=2000,
                random_state=42
            )
            nn_model.fit(X_train, y_train)
            models['neural_network'] = nn_model
            
            logger.info(f"Built {len(models)} individual models")
            return models
            
        except Exception as e:
            logger.error(f"‚ùå Individual model building failed: {e}")
            return None
    
    def _validate_individual_models(self, models, data_splits):
        """Validate individual models"""
        try:
            X_val = data_splits['X_val']
            y_val = data_splits['y_val']
            
            validation_results = {}
            all_passed = True
            
            for name, model in models.items():
                # Get predictions
                y_pred_proba = model.predict_proba(X_val)[:, 1]
                y_pred = (y_pred_proba > 0.5).astype(int)
                
                # Calculate metrics
                accuracy = accuracy_score(y_val, y_pred)
                logloss = log_loss(y_val, y_pred_proba)
                auc = roc_auc_score(y_val, y_pred_proba)
                brier = brier_score_loss(y_val, y_pred_proba)
                
                # Check thresholds
                passed = (
                    accuracy >= self.min_accuracy_threshold and
                    auc >= self.min_auc_threshold and
                    logloss <= self.max_logloss_threshold
                )
                
                validation_results[name] = {
                    'accuracy': accuracy,
                    'log_loss': logloss,
                    'auc_roc': auc,
                    'brier_score': brier,
                    'passed': passed
                }
                
                if not passed:
                    all_passed = False
                    logger.warning(f"‚ö†Ô∏è {name} failed validation: acc={accuracy:.3f}, auc={auc:.3f}, logloss={logloss:.3f}")
                else:
                    logger.info(f"‚úÖ {name} passed validation: acc={accuracy:.3f}, auc={auc:.3f}, logloss={logloss:.3f}")
            
            if not all_passed:
                return {'passed': False, 'reason': 'One or more individual models failed validation'}
            
            return {'passed': True, 'results': validation_results}
            
        except Exception as e:
            return {'passed': False, 'reason': f'Validation error: {e}'}
    
    def _build_ensemble_model(self, individual_models, data_splits):
        """Build ensemble model"""
        try:
            X_train = data_splits['X_train']
            y_train = data_splits['y_train']
            X_val = data_splits['X_val']
            y_val = data_splits['y_val']
            
            # Get predictions from all models
            predictions = {}
            for name, model in individual_models.items():
                pred_proba = model.predict_proba(X_val)[:, 1]
                predictions[name] = pred_proba
            
            # Optimize ensemble weights
            best_weights = self._optimize_ensemble_weights(predictions, y_val)
            
            # Create ensemble model
            ensemble_model = EnsembleModel(individual_models, best_weights)
            
            logger.info(f"Built ensemble model with weights: {best_weights}")
            return ensemble_model
            
        except Exception as e:
            logger.error(f"‚ùå Ensemble model building failed: {e}")
            return None
    
    def _optimize_ensemble_weights(self, predictions, y_val):
        """Optimize ensemble weights using validation data"""
        def objective(trial):
            # Generate weights for each model
            weights = []
            model_names = list(predictions.keys())
            
            for i, model_name in enumerate(model_names):
                if i == len(model_names) - 1:
                    # Last weight is 1 - sum of others
                    weight = 1 - sum(weights)
                else:
                    weight = trial.suggest_float(f'weight_{model_name}', 0.1, 0.9)
                weights.append(weight)
            
            # Normalize weights
            weights = np.array(weights)
            weights = weights / weights.sum()
            
            # Weighted ensemble prediction
            ensemble_pred = sum(w * pred for w, pred in zip(weights, predictions.values()))
            
            # Calculate log loss
            score = log_loss(y_val, ensemble_pred)
            
            return score
        
        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=50)
        
        # Create weight dictionary
        model_names = list(predictions.keys())
        weights = {}
        for i, model_name in enumerate(model_names):
            if i == len(model_names) - 1:
                weight = 1 - sum(weights.values())
            else:
                weight = study.best_params[f'weight_{model_name}']
            weights[model_name] = weight
        
        logger.info(f"Optimized ensemble weights: {weights}")
        return weights
    
    def _validate_ensemble_model(self, ensemble_model, data_splits):
        """Validate ensemble model"""
        try:
            X_val = data_splits['X_val']
            y_val = data_splits['y_val']
            X_test = data_splits['X_test']
            y_test = data_splits['y_test']
            
            # Validation set performance
            y_pred_proba_val = ensemble_model.predict_proba(X_val)[:, 1]
            y_pred_val = (y_pred_proba_val > 0.5).astype(int)
            
            val_accuracy = accuracy_score(y_val, y_pred_val)
            val_logloss = log_loss(y_val, y_pred_proba_val)
            val_auc = roc_auc_score(y_val, y_pred_proba_val)
            val_brier = brier_score_loss(y_val, y_pred_proba_val)
            
            # Test set performance (only if test data exists)
            if len(X_test) > 0:
                y_pred_proba_test = ensemble_model.predict_proba(X_test)[:, 1]
                y_pred_test = (y_pred_proba_test > 0.5).astype(int)
                
                test_accuracy = accuracy_score(y_test, y_pred_test)
                test_logloss = log_loss(y_test, y_pred_proba_test)
                test_auc = roc_auc_score(y_test, y_pred_proba_test)
                test_brier = brier_score_loss(y_test, y_pred_proba_test)
                
                # Check thresholds with test data
                passed = (
                    val_accuracy >= self.min_accuracy_threshold and
                    val_auc >= self.min_auc_threshold and
                    val_logloss <= self.max_logloss_threshold and
                    test_accuracy >= self.min_accuracy_threshold and
                    test_auc >= self.min_auc_threshold
                )
            else:
                # No test data, only validate on validation set
                test_accuracy = test_logloss = test_auc = test_brier = 0
                passed = (
                    val_accuracy >= self.min_accuracy_threshold and
                    val_auc >= self.min_auc_threshold and
                    val_logloss <= self.max_logloss_threshold
                )
            
            if not passed:
                return {'passed': False, 'reason': 'Ensemble model failed validation thresholds'}
            
            logger.info(f"‚úÖ Ensemble validation: val_acc={val_accuracy:.3f}, test_acc={test_accuracy:.3f}")
            logger.info(f"‚úÖ Ensemble AUC: val_auc={val_auc:.3f}, test_auc={test_auc:.3f}")
            
            return {
                'passed': True,
                'validation': {
                    'accuracy': val_accuracy, 'log_loss': val_logloss,
                    'auc_roc': val_auc, 'brier_score': val_brier
                },
                'test': {
                    'accuracy': test_accuracy, 'log_loss': test_logloss,
                    'auc_roc': test_auc, 'brier_score': test_brier
                }
            }
            
        except Exception as e:
            return {'passed': False, 'reason': f'Ensemble validation error: {e}'}
    
    def _save_models(self, individual_models, ensemble_model, validation_results, ensemble_validation):
        """Save all models and results"""
        # Save individual models
        for name, model in individual_models.items():
            with open(self.output_dir / f'{name}_model.pkl', 'wb') as f:
                pickle.dump(model, f)
        
        # Save ensemble model
        with open(self.output_dir / 'ensemble_model.pkl', 'wb') as f:
            pickle.dump(ensemble_model, f)
        
        # Save validation results
        results = {
            'timestamp': datetime.now().isoformat(),
            'individual_models': validation_results['results'],
            'ensemble_validation': ensemble_validation,
            'model_metadata': {
                'feature_count': len(individual_models['xgboost'].feature_importances_),
                'training_games': 2139,  # From previous output
                'validation_games': 272,  # From previous output
                'test_games': 0  # From previous output
            }
        }
        
        with open(self.output_dir / 'model_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"Models and results saved to {self.output_dir}")

class EnsembleModel:
    """Ensemble model combining multiple base models"""
    
    def __init__(self, models, weights):
        self.models = models
        self.weights = weights
    
    def predict_proba(self, X):
        """Predict probabilities using ensemble"""
        predictions = []
        
        for name, model in self.models.items():
            pred_proba = model.predict_proba(X)[:, 1]
            predictions.append(pred_proba)
        
        # Weighted average
        ensemble_pred = sum(w * pred for w, pred in zip(self.weights.values(), predictions))
        
        # Convert to probability matrix
        return np.column_stack([1 - ensemble_pred, ensemble_pred])

if __name__ == "__main__":
    builder = ComprehensiveModelBuilder()
    models = builder.build_comprehensive_models_with_validation()
    
    if models is not None:
        logger.info("üéâ Phase 3 Model Architecture: SUCCESS")
        logger.info("Ready to proceed to Phase 4: Validation Framework")
    else:
        logger.error("‚ùå Phase 3 Model Architecture: FAILED")
        logger.error("Fix model issues before proceeding")
