# üèóÔ∏è Q3: Model Architecture - Complete Synthesis
## Building Production-Ready NFL Betting Systems: Mathematical Foundations to Implementation

---

## üìä Executive Summary

After analyzing Claude's production-focused two-stage ensemble, Gemini's survey of 7 cutting-edge architectures, and GPT-4's rigorous mathematical foundations, a clear implementation path emerges. The consensus: **start with a robust two-stage ensemble (proven profitable), then selectively add advanced techniques based on data availability and specific edge requirements**.

**GPT-4's Critical Mathematical Insight**: The two-stage architecture is not just an engineering choice but is mathematically optimal. The bias-variance decomposition proves that ensemble error = average base error - diversity, while the separation of prediction and calibration optimizes different mathematical objectives independently.

**Critical Insight**: All three AIs emphasize that **probability calibration is more important than raw accuracy**. GPT-4's theoretical analysis proves that proper scoring rules (log-loss, Brier) align with Kelly-optimal betting, making calibration mathematically essential for profitable betting.

**Architecture Evolution Path**:
1. **Foundation**: LightGBM + XGBoost ensemble with isotonic calibration (GPT-4 proves optimality)
2. **Enhancement**: Add specialized experts (MoE) for different game contexts
3. **Advanced**: Incorporate Transformers for sequential data when available
4. **Future**: Explore GNNs for relational patterns and meta-learning for adaptation

**GPT-4's Mathematical Validation**:
- Ensemble variance reduction: œÉ¬≤/N for N independent models
- Kelly criterion integration: aligns loss functions with log-wealth growth
- TreeSHAP complexity: O(T√óL√óD¬≤) makes real-time explanation feasible
- Online learning convergence: O(1/‚àöT) guarantees for weekly updates

---

## üéØ Stage 1: Foundation Architecture (Start Here)

### The Two-Stage Ensemble Design

#### Base Models Layer
```python
class BaseModelEnsemble:
    def __init__(self):
        # Primary model (Claude's recommendation)
        self.lgb_model = LGBMClassifier(
            n_estimators=500,
            learning_rate=0.03,
            max_depth=7,
            min_child_samples=50,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            objective='binary',
            metric='binary_logloss'
        )

        # Secondary model for diversity
        self.xgb_model = XGBClassifier(
            n_estimators=500,
            learning_rate=0.03,
            max_depth=7,
            min_child_weight=5,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            objective='binary:logistic',
            eval_metric='logloss'
        )

        # Optional: CatBoost for categorical features
        self.cat_model = CatBoostClassifier(
            iterations=500,
            learning_rate=0.03,
            depth=7,
            l2_leaf_reg=3,
            verbose=False
        )

    def fit(self, X_train, y_train, X_val, y_val):
        """Train all base models"""
        # LightGBM
        self.lgb_model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            callbacks=[early_stopping(50), log_evaluation(0)]
        )

        # XGBoost
        self.xgb_model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            early_stopping_rounds=50,
            verbose=False
        )

        # CatBoost
        self.cat_model.fit(
            X_train, y_train,
            eval_set=(X_val, y_val),
            early_stopping_rounds=50,
            verbose=False
        )

    def predict_proba(self, X):
        """Weighted ensemble prediction"""
        # Get individual predictions
        lgb_pred = self.lgb_model.predict_proba(X)[:, 1]
        xgb_pred = self.xgb_model.predict_proba(X)[:, 1]
        cat_pred = self.cat_model.predict_proba(X)[:, 1]

        # Weighted average (weights learned via validation)
        weights = [0.45, 0.35, 0.20]  # LGB, XGB, Cat
        ensemble_pred = (
            weights[0] * lgb_pred +
            weights[1] * xgb_pred +
            weights[2] * cat_pred
        )

        return ensemble_pred
```

#### Calibration Layer (Critical for Betting)
```python
class ProbabilityCalibrator:
    def __init__(self, method='isotonic'):
        """
        Isotonic regression is preferred for betting
        Platt scaling as backup for small samples
        """
        if method == 'isotonic':
            self.calibrator = IsotonicRegression(
                y_min=0.0,
                y_max=1.0,
                out_of_bounds='clip'
            )
        else:  # platt
            self.calibrator = LogisticRegression(
                solver='lbfgs',
                max_iter=1000
            )

    def fit(self, probabilities, outcomes):
        """Fit calibration mapping"""
        self.calibrator.fit(probabilities, outcomes)

    def calibrate(self, probabilities):
        """Apply calibration"""
        return self.calibrator.transform(probabilities)

    def plot_reliability_diagram(self, probs, outcomes):
        """Visual calibration check"""
        from sklearn.calibration import calibration_curve

        fraction_pos, mean_pred = calibration_curve(
            outcomes, probs, n_bins=10, strategy='uniform'
        )

        plt.figure(figsize=(10, 6))
        plt.plot(mean_pred, fraction_pos, 's-', label='Model')
        plt.plot([0, 1], [0, 1], 'k--', label='Perfect')
        plt.xlabel('Mean Predicted Probability')
        plt.ylabel('Fraction of Positives')
        plt.legend()
        plt.title('Reliability Diagram')
        plt.show()

        # Calculate ECE (Expected Calibration Error)
        ece = np.mean(np.abs(fraction_pos - mean_pred))
        print(f"ECE: {ece:.4f} (target: <0.05)")
        return ece
```

### Feature Engineering Pipeline

#### NFL-Specific Features (Both AIs Emphasize)
```python
class NFLFeatureEngineer:
    def __init__(self):
        self.features = {}

    def create_efficiency_metrics(self, df):
        """DVOA and EPA - most predictive features"""
        features = {}

        # DVOA v8.0 (0.457 correlation to performance)
        features['home_dvoa_offense'] = df['home_pass_dvoa'] * 0.6 + df['home_rush_dvoa'] * 0.4
        features['away_dvoa_offense'] = df['away_pass_dvoa'] * 0.6 + df['away_rush_dvoa'] * 0.4
        features['home_dvoa_defense'] = df['home_pass_dvoa_def'] * 0.6 + df['home_rush_dvoa_def'] * 0.4
        features['away_dvoa_defense'] = df['away_pass_dvoa_def'] * 0.6 + df['away_rush_dvoa_def'] * 0.4

        # Weighted EPA (14% more predictive than standard)
        features['home_epa_weighted'] = (
            df['home_epa_pass'] * df['home_pass_rate'] +
            df['home_epa_rush'] * (1 - df['home_pass_rate'])
        )
        features['away_epa_weighted'] = (
            df['away_epa_pass'] * df['away_pass_rate'] +
            df['away_epa_rush'] * (1 - df['away_pass_rate'])
        )

        # Success rate
        features['home_success_rate'] = df['home_successful_plays'] / df['home_total_plays']
        features['away_success_rate'] = df['away_successful_plays'] / df['away_total_plays']

        return pd.DataFrame(features)

    def create_situational_features(self, df):
        """Context matters in NFL"""
        features = {}

        # Rest advantage
        features['home_rest_days'] = df['home_days_since_last_game']
        features['away_rest_days'] = df['away_days_since_last_game']
        features['rest_differential'] = features['home_rest_days'] - features['away_rest_days']

        # Divisional games (different dynamics)
        features['is_divisional'] = df['is_divisional'].astype(int)

        # Travel distance (log-transformed)
        features['away_travel_distance_log'] = np.log1p(df['away_travel_distance'])

        # Time zones crossed
        features['timezone_difference'] = df['away_timezone_diff'].abs()

        # Primetime games (public money influence)
        features['is_primetime'] = df['game_time'].isin(['SNF', 'MNF', 'TNF']).astype(int)

        return pd.DataFrame(features)

    def create_weather_features(self, df):
        """Weather impact (especially wind)"""
        features = {}

        # Wind is most impactful (>20mph reduces passing 25%)
        features['wind_speed'] = df['wind_speed']
        features['high_wind'] = (df['wind_speed'] > 20).astype(int)

        # Temperature extremes
        features['temperature'] = df['temperature']
        features['extreme_cold'] = (df['temperature'] < 32).astype(int)

        # Precipitation
        features['has_precipitation'] = df['precipitation'].notna().astype(int)

        # Dome games (no weather impact)
        features['is_dome'] = df['stadium_type'] == 'dome'

        return pd.DataFrame(features)

    def create_market_features(self, df):
        """Sharp money indicators (Gemini emphasis)"""
        features = {}

        # Reverse line movement (RLM)
        features['reverse_line_movement'] = (
            (df['line_movement'] * df['money_percentage_differential']) < 0
        ).astype(int)

        # Sharp money indicator (>20% bet/money divergence)
        features['sharp_money'] = (
            np.abs(df['bet_percentage'] - df['money_percentage']) > 20
        ).astype(int)

        # Steam moves
        features['steam_move'] = df['rapid_line_change'].astype(int)

        # Public fade opportunity
        features['fade_public'] = (df['public_percentage'] > 70).astype(int)

        return pd.DataFrame(features)
```

### Custom Betting Loss Function

```python
class BettingOptimizedLoss:
    """
    Combines log loss with expected value optimization
    """
    def __init__(self, odds_column='decimal_odds'):
        self.odds_column = odds_column

    def custom_objective(self, y_true, y_pred):
        """
        XGBoost/LightGBM custom objective
        """
        # Standard log loss gradient
        grad = y_pred - y_true

        # Adjust for betting value
        odds = self.get_odds_for_batch()
        ev_weight = self.calculate_ev_weight(y_pred, odds)

        # Weight gradient by expected value
        grad = grad * ev_weight

        # Hessian (second derivative)
        hess = y_pred * (1 - y_pred) * ev_weight

        return grad, hess

    def calculate_ev_weight(self, prob, odds):
        """
        Weight samples by their expected value
        """
        ev = prob * (odds - 1) - (1 - prob)

        # Exponential weighting for high EV bets
        weight = np.exp(ev / 0.1)  # Temperature parameter

        # Normalize
        weight = weight / weight.mean()

        return weight
```

---

## üöÄ Stage 2: Production Optimization

### Speed Optimization with ONNX

```python
class ONNXOptimizer:
    def __init__(self, model):
        self.model = model

    def convert_to_onnx(self):
        """
        Convert model to ONNX for 2-5x speedup
        """
        import onnx
        from skl2onnx import to_onnx

        # Get sample input for shape inference
        sample_input = self.get_sample_input()

        # Convert
        onnx_model = to_onnx(
            self.model,
            sample_input,
            target_opset=12
        )

        # Save
        onnx.save_model(onnx_model, 'model.onnx')

        return onnx_model

    def create_inference_session(self):
        """
        Create optimized inference session
        """
        import onnxruntime as ort

        # Create session with optimization
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

        # Use all available cores
        sess_options.intra_op_num_threads = os.cpu_count()

        session = ort.InferenceSession(
            'model.onnx',
            sess_options,
            providers=['CPUExecutionProvider']
        )

        return session

    def predict_batch(self, session, X_batch):
        """
        Fast batch prediction (<100ms for 1000 games)
        """
        input_name = session.get_inputs()[0].name
        output_name = session.get_outputs()[0].name

        predictions = session.run(
            [output_name],
            {input_name: X_batch.astype(np.float32)}
        )[0]

        return predictions
```

### Redis Caching Layer

```python
class FeatureCache:
    def __init__(self):
        import redis
        self.redis_client = redis.Redis(
            host='localhost',
            port=6379,
            db=0,
            decode_responses=False
        )

    def cache_features(self, game_id, features):
        """
        Cache computed features (1 hour TTL)
        """
        import pickle

        key = f"features:{game_id}"
        value = pickle.dumps(features)

        self.redis_client.setex(
            key,
            3600,  # 1 hour TTL
            value
        )

    def get_cached_features(self, game_id):
        """
        Sub-millisecond retrieval
        """
        import pickle

        key = f"features:{game_id}"
        value = self.redis_client.get(key)

        if value:
            return pickle.loads(value)
        return None

    def cache_predictions(self, game_id, predictions):
        """
        Cache model predictions (5 minute TTL)
        """
        key = f"predictions:{game_id}"
        value = json.dumps({
            'probability': float(predictions['probability']),
            'confidence': float(predictions['confidence']),
            'timestamp': datetime.now().isoformat()
        })

        self.redis_client.setex(key, 300, value)
```

### Online Learning with River

```python
class OnlineLearner:
    def __init__(self):
        from river import ensemble
        from river import tree

        # Adaptive Random Forest for online learning
        self.model = ensemble.AdaptiveRandomForestClassifier(
            n_models=10,
            max_features='sqrt',
            lambda_value=6,
            grace_period=50,
            split_confidence=0.01,
            tie_threshold=0.05
        )

    def update_single(self, features, outcome):
        """
        Update model with single game result
        """
        self.model.learn_one(features, outcome)

    def update_batch(self, X_batch, y_batch):
        """
        Weekly batch updates
        """
        for x, y in zip(X_batch, y_batch):
            self.model.learn_one(dict(x), y)

    def predict(self, features):
        """
        Get updated prediction
        """
        return self.model.predict_proba_one(features)
```

---

## üîÆ Stage 3: Advanced Architectures (Future Enhancements)

### Mixture of Experts for Context

```python
class NFLMixtureOfExperts:
    def __init__(self):
        self.experts = {
            'early_season': self.create_early_season_expert(),
            'divisional': self.create_divisional_expert(),
            'primetime': self.create_primetime_expert(),
            'weather': self.create_weather_expert(),
            'late_season': self.create_late_season_expert()
        }

        # Gating network
        self.gate = self.create_gating_network()

    def create_early_season_expert(self):
        """Expert for Weeks 1-4 uncertainty"""
        return LGBMClassifier(
            n_estimators=300,
            learning_rate=0.05,  # Higher learning rate for volatility
            max_depth=5,  # Shallower to avoid overfitting
            min_child_samples=100  # More conservative
        )

    def create_gating_network(self):
        """Decides which experts to use"""
        return Sequential([
            Dense(64, activation='relu'),
            Dropout(0.3),
            Dense(32, activation='relu'),
            Dense(len(self.experts), activation='softmax')
        ])

    def predict(self, X, context):
        """
        Route to appropriate experts
        """
        # Get expert weights from gate
        weights = self.gate.predict(context)

        # Get predictions from each expert
        expert_preds = {}
        for name, expert in self.experts.items():
            expert_preds[name] = expert.predict_proba(X)[:, 1]

        # Weighted combination
        final_pred = np.zeros(len(X))
        for i, name in enumerate(self.experts.keys()):
            final_pred += weights[:, i] * expert_preds[name]

        return final_pred
```

### Transformer for Sequential Data (When Available)

```python
class NFLTransformer:
    """
    For modeling play-by-play sequences
    """
    def __init__(self, sequence_length=50):
        self.sequence_length = sequence_length
        self.model = self.build_transformer()

    def build_transformer(self):
        """
        Transformer architecture for game sequences
        """
        from tensorflow.keras.layers import MultiHeadAttention

        inputs = Input(shape=(self.sequence_length, 25))  # 25 features per play

        # Positional encoding
        positions = tf.range(start=0, limit=self.sequence_length, delta=1)
        position_embeddings = Embedding(
            input_dim=self.sequence_length,
            output_dim=64
        )(positions)

        x = inputs + position_embeddings

        # Multi-head attention
        attention = MultiHeadAttention(
            num_heads=8,
            key_dim=64,
            dropout=0.1
        )
        x = attention(x, x)
        x = Dropout(0.1)(x)
        x = LayerNormalization(epsilon=1e-6)(x)

        # Feed forward
        ff = Sequential([
            Dense(256, activation='relu'),
            Dropout(0.1),
            Dense(64)
        ])
        x = ff(x)
        x = Dropout(0.1)(x)
        x = LayerNormalization(epsilon=1e-6)(x)

        # Global pooling
        x = GlobalAveragePooling1D()(x)

        # Output
        outputs = Dense(1, activation='sigmoid')(x)

        return Model(inputs=inputs, outputs=outputs)

    def prepare_sequences(self, game_data):
        """
        Convert game to sequence of plays
        """
        sequences = []

        for play in game_data['plays']:
            features = [
                play['down'],
                play['distance'],
                play['field_position'],
                play['score_differential'],
                play['time_remaining'],
                # ... more play features
            ]
            sequences.append(features)

        # Pad or truncate to fixed length
        if len(sequences) < self.sequence_length:
            sequences = self.pad_sequences(sequences)
        else:
            sequences = sequences[-self.sequence_length:]

        return np.array(sequences)
```

### Graph Neural Network for Relationships

```python
class NFLGraphNetwork:
    """
    Model team relationships and matchups
    """
    def __init__(self):
        import torch_geometric
        from torch_geometric.nn import GCNConv, global_mean_pool

        self.conv1 = GCNConv(32, 64)
        self.conv2 = GCNConv(64, 32)
        self.conv3 = GCNConv(32, 16)

    def create_league_graph(self, season_data):
        """
        Teams as nodes, games as edges
        """
        nodes = []  # Team features
        edges = []  # Game relationships
        edge_weights = []  # Point differentials

        for game in season_data:
            home_idx = self.team_to_idx[game['home_team']]
            away_idx = self.team_to_idx[game['away_team']]

            # Bidirectional edges
            edges.append([home_idx, away_idx])
            edges.append([away_idx, home_idx])

            # Weight by margin of victory
            margin = game['home_score'] - game['away_score']
            edge_weights.extend([margin, -margin])

        return nodes, edges, edge_weights

    def forward(self, x, edge_index, batch):
        """
        GNN forward pass
        """
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=0.3, training=self.training)
        x = F.relu(self.conv2(x, edge_index))
        x = F.dropout(x, p=0.3, training=self.training)
        x = self.conv3(x, edge_index)

        # Global pooling for graph-level prediction
        x = global_mean_pool(x, batch)

        return F.sigmoid(x)
```

---

## üìä Monitoring and Drift Detection

### Population Stability Index (PSI)

```python
class DriftMonitor:
    def __init__(self, reference_data):
        self.reference_data = reference_data
        self.psi_threshold = {
            'no_action': 0.1,
            'monitor': 0.2,
            'retrain': 0.25
        }

    def calculate_psi(self, current_data, feature):
        """
        PSI for single feature
        """
        # Create bins from reference data
        _, bins = pd.qcut(
            self.reference_data[feature],
            q=10,
            retbins=True,
            duplicates='drop'
        )

        # Calculate distributions
        ref_counts = pd.cut(self.reference_data[feature], bins).value_counts()
        curr_counts = pd.cut(current_data[feature], bins).value_counts()

        # Normalize
        ref_prop = (ref_counts + 1) / (len(self.reference_data) + 10)
        curr_prop = (curr_counts + 1) / (len(current_data) + 10)

        # PSI calculation
        psi = np.sum(
            (curr_prop - ref_prop) * np.log(curr_prop / ref_prop)
        )

        return psi

    def get_action(self, psi):
        """
        Determine action based on PSI
        """
        if psi < self.psi_threshold['no_action']:
            return "‚úÖ No action needed"
        elif psi < self.psi_threshold['monitor']:
            return "üü° Monitor closely"
        elif psi < self.psi_threshold['retrain']:
            return "üü† Consider retraining"
        else:
            return "üî¥ Immediate retrain required"
```

---

## üö¶ Production API Server

### FastAPI Implementation

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import asyncio
from typing import Optional

app = FastAPI(title="NFL Betting Model API")

class GameRequest(BaseModel):
    home_team: str
    away_team: str
    week: int
    season: int
    weather: Optional[dict] = None
    injuries: Optional[dict] = None

class PredictionResponse(BaseModel):
    probability: float
    confidence: float
    recommended_bet: float
    edge: float
    clv_estimate: float
    warnings: list

@app.post("/predict", response_model=PredictionResponse)
async def predict_game(request: GameRequest):
    """
    Main prediction endpoint
    """
    try:
        # Check cache first
        cached = await get_cached_prediction(request)
        if cached:
            return cached

        # Extract features
        features = await extract_features(request)

        # Get prediction
        probability = model.predict_proba(features)[0, 1]

        # Calibrate
        calibrated_prob = calibrator.calibrate(probability)

        # Calculate confidence (via prediction interval width)
        confidence = calculate_confidence(features)

        # Calculate Kelly stake
        odds = await get_current_odds(request)
        edge = calculate_edge(calibrated_prob, odds)
        kelly_stake = calculate_kelly(calibrated_prob, odds)

        # Estimate CLV
        clv_estimate = estimate_clv(request, calibrated_prob)

        # Check for warnings
        warnings = check_warnings(features, confidence)

        response = PredictionResponse(
            probability=calibrated_prob,
            confidence=confidence,
            recommended_bet=kelly_stake,
            edge=edge,
            clv_estimate=clv_estimate,
            warnings=warnings
        )

        # Cache result
        await cache_prediction(request, response)

        return response

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """
    System health check
    """
    return {
        "status": "healthy",
        "model_version": MODEL_VERSION,
        "last_update": LAST_UPDATE,
        "drift_status": check_drift_status(),
        "cache_status": check_cache_status()
    }

@app.get("/metrics")
async def get_metrics():
    """
    Performance metrics dashboard
    """
    return {
        "last_week_roi": calculate_recent_roi(7),
        "last_month_roi": calculate_recent_roi(30),
        "season_roi": calculate_season_roi(),
        "clv_average": get_average_clv(),
        "beat_close_rate": get_beat_close_rate(),
        "sharpe_ratio": calculate_sharpe(),
        "max_drawdown": calculate_max_drawdown()
    }
```

---

## üìã Implementation Roadmap

### Phase 1: MVP (Week 1-2)
```python
mvp_checklist = {
    'base_models': {
        'lightgbm': False,
        'xgboost': False,
        'ensemble_weights': False
    },
    'calibration': {
        'isotonic_regression': False,
        'reliability_diagram': False,
        'ece_calculation': False
    },
    'features': {
        'efficiency_metrics': False,
        'situational_features': False,
        'weather_features': False
    },
    'api': {
        'prediction_endpoint': False,
        'health_check': False,
        'basic_caching': False
    }
}
```

### Phase 2: Optimization (Week 3-4)
```python
optimization_checklist = {
    'performance': {
        'onnx_conversion': False,
        'redis_caching': False,
        'batch_processing': False
    },
    'monitoring': {
        'psi_calculation': False,
        'drift_alerts': False,
        'performance_dashboard': False
    },
    'risk': {
        'kelly_optimization': False,
        'confidence_intervals': False,
        'var_estimation': False
    }
}
```

### Phase 3: Advanced Features (Month 2)
```python
advanced_checklist = {
    'architectures': {
        'mixture_of_experts': False,
        'online_learning': False,
        'transformer_prep': False
    },
    'features': {
        'market_indicators': False,
        'player_tracking': False,
        'sentiment_analysis': False
    },
    'deployment': {
        'kubernetes': False,
        'auto_scaling': False,
        'ab_testing': False
    }
}
```

---

## üéØ Performance Benchmarks

### Model Performance Targets
- **Accuracy**: 54-58% (profitable range)
- **ECE**: <5% (calibration error)
- **AUC**: >0.58 (discrimination)
- **Brier Score**: <0.24

### System Performance Targets
- **Latency**: <100ms P95
- **Throughput**: 1000+ predictions/sec
- **Cache Hit Rate**: >80%
- **Uptime**: 99.9%

### Betting Performance Targets
- **ROI**: 2-5% per bet
- **CLV**: >2% average
- **Sharpe**: >1.5
- **Max Drawdown**: <20%

---

## üí° Key Implementation Insights

### From Claude (Production Focus):
1. **LightGBM > XGBoost** for primary model (faster)
2. **Isotonic > Platt** for calibration (better for betting)
3. **25% Kelly** is optimal fraction
4. **ONNX** provides 2-5x speedup
5. **Redis caching** is essential for <100ms latency

### From Gemini (Innovation Focus):
1. **MoE** handles context better than single model
2. **Transformers** capture game flow when sequential data available
3. **GNNs** model team relationships uniquely
4. **Meta-learning** solves cold start problem
5. **Causal models** prevent spurious correlations

### Synthesis Wisdom:
1. **Start simple, add complexity gradually**
2. **Calibration matters more than accuracy**
3. **Monitor drift continuously**
4. **Cache everything possible**
5. **Test in production with small stakes**

---

## üöÄ Quick Start Commands

```bash
# Install dependencies
pip install -r requirements.txt

# Train base models
python train_models.py --config config/base_ensemble.yaml

# Calibrate probabilities
python calibrate.py --model models/ensemble.pkl

# Convert to ONNX
python optimize.py --convert-onnx --model models/calibrated.pkl

# Start API server
uvicorn api.server:app --host 0.0.0.0 --port 8000 --workers 4

# Deploy with Docker
docker build -t nfl-model:latest .
docker run -p 8000:8000 nfl-model:latest

# Monitor drift
python monitor.py --check-drift --alert-threshold 0.2
```

---

## üèÜ Conclusion

The synthesis reveals a clear path: **build a rock-solid two-stage ensemble first**, then selectively add advanced techniques based on specific needs and data availability.

Claude provides the production-ready foundation with specific implementation details, while Gemini offers a roadmap for future enhancements with cutting-edge architectures.

The key insight both share: **probability calibration is the difference between a research model and a profitable betting system**. A simpler, well-calibrated model will outperform a complex, poorly calibrated one in real betting scenarios.

Start with the foundation architecture, optimize for production, then gradually incorporate advanced techniques as your data and expertise grow. Remember: the goal isn't the most sophisticated model, but the most profitable one.