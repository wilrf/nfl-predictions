Feature Discovery: A Theoretical Framework for
NFL Spread Prediction
1. Feature Selection Under Multicollinearity
Filter vs. Wrapper Methods: In high-dimensional data (500+ features for ~2,500 NFL games), identifying
the most predictive features requires careful selection methods, especially under multicollinearity (many
features conveying overlapping information). Filter methods rank features by some univariate criterion
(independent of any specific model). Examples include Pearson correlation (linear correlation with the
target) and mutual information (general statistical dependence with the target). These are fast but ignore
interactions between features. In contrast, wrapper methods like recursive feature elimination (RFE) use a
predictive model to evaluate subsets of features, iteratively removing the least useful feature and retraining
the model until an optimal subset is found. RFE can account for feature interactions and multicollinearity by
evaluating actual model performance, but it is computationally expensive (training many models) and risks
overfitting if not properly cross-validated.
Pearson Correlation vs. Mutual Information: Pearson’s correlation coefficient $\rho_{X,Y}$ measures
linear association: $\rho_{X,Y} = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}$. It detects a linear
relationship (or monotonic, if using Spearman’s rank correlation) but can miss non-linear predictive
patterns. For example, if the outcome $Y$ has a quadratic or threshold relationship with feature $X$,
Pearson correlation may be near zero even though $X$ is predictive of $Y$ 1 . Mutual information (MI),
defined as $I(X;Y) = H(Y) - H(Y\mid X)$, measures any dependence between $X$ and $Y$, not just linear
correlation. $I(X;Y)=0$ if and only if $X$ and $Y$ are statistically independent. Thus, MI can capture nonlinear or conditional relationships that correlation misses – e.g. a feature that only matters in extreme
weather conditions might have zero linear correlation with win margin but non-zero MI because it
sometimes strongly affects the outcome. In a binary classification (cover vs. not cover) near 50/50 (as in
betting at -110 odds), mutual information is often more sensitive to subtle signals than correlation
because it directly quantifies reduction in outcome uncertainty 2 . A small MI means the feature provides a
small edge over random guessing, which is crucial when needing >52.4% accuracy to beat the odds. In
practice, however, MI estimation is non-parametric and requires large sample sizes to be reliable 3 . With
limited data (~2.5k samples), naive MI estimates can be noisy. Pearson correlation, being simpler, is more
stable in low-sample regimes but risks missing non-linear effects. When each method is optimal: Use
correlation for a quick check and when you expect roughly linear or monotonic relationships (e.g. yards per
play vs. win probability) or need to screen out redundancies (high inter-feature correlations). Use mutual
information when you suspect non-linear dependencies or categorical interactions (e.g. a certain
combination of conditions yields wins) – MI will outperform correlation in detecting these (e.g. it flags a
quadratic $X^2$ relation that correlation would deem uncorrelated 1 ). In summary, correlation is a fast
filter for linear signals, while mutual information is a powerful filter for any dependence (at the cost of
higher sample requirements and computation).
Handling Redundant Features: Multicollinearity means several features convey the same information.
Including all of them gives diminishing returns and can confuse certain models (e.g. logistic regression

1

coefficients become unstable due to high variance inflation). A practical strategy is to first remove or
cluster highly correlated features using a simple filter, then apply more advanced selection on the reduced
set 4 . For example, if offensive yards per play and offensive EPA/play are 95% correlated, one of them can
be dropped as redundant. Variance Inflation Factor (VIF) analysis in a logistic regression context can also
identify multicollinearity – a high VIF indicates a feature largely explained by others. By removing “straight
correlations” first, we ensure remaining features add unique information 4 . After this, more sophisticated
criteria can rank the survivors.
Recursive Feature Elimination (RFE): RFE is a wrapper that repeatedly trains a model (e.g. XGBoost or
logistic regression) and removes the least important feature each round. Importance can be defined by
model coefficients (for linear models) or feature importance scores (for tree ensembles). RFE shines when
feature interactions or complex joint effects exist, because it evaluates features in the context of others.
For example, a feature might have little standalone correlation with the target but become useful in
combination with others; RFE using a tree ensemble could detect this because the model’s performance
improves only when the feature is present with its complements. RFE will tend to keep one out of a group of
highly collinear features – once one feature is in the model, the others in that group contribute little
additional performance and thus get eliminated. In other words, the model “picks” one representative of a
redundant cluster. This addresses multicollinearity by letting the model decide which feature carries the
shared information. RFE is often optimal when model performance is the priority and computational
cost is acceptable: it directly optimizes for the subset that maximizes predictive accuracy (subject to the
greedy search). It’s particularly useful with powerful non-linear models (like gradient boosting) that can
capture interactions – RFE will naturally keep features involved in strong interactions because removing
them causes a drop in performance. However, RFE can be computationally heavy (training many models)
and prone to overfitting if one isn’t careful (e.g. doing RFE on the entire dataset without cross-validation
could select features that exploit random quirks). In practice, one might combine methods: e.g. correlation
filtering to remove duplicates (multicollinearity reduction), then a mutual information filter to pick the
top, say, 50 candidates by relevance to target, and finally RFE or an embedded method (like tree-based
feature importance or regularized regression) to choose the best ~20 features out of those. This ensemble
approach leverages the strengths of each method 5 6 .
Summary: Pearson correlation is fast and effective for linear signals and eliminating obvious duplicates;
mutual information can catch any predictive relationship and is more appropriate when the form of
dependence is unknown (e.g. binary outcome with complex triggers), while RFE and other wrapper/
embedded methods (like Relief-F or Boruta) actually use the model to find an optimal feature subset,
accounting for multicollinearity and interactions. No single method is universally best – “no free lunch”
applies 6 . In practice, use correlation/VIF to address multicollinearity first, then apply MI or modelbased selection. If computational resources allow, RFE with cross-validation can yield a feature set that
maximizes spread-covering prediction accuracy.

2. Detecting Feature Interactions: Friedman’s H-Statistic vs. SHAP
A key advantage of tree-based models (XGBoost, LightGBM) is their ability to capture feature interactions –
situations where the joint effect of two (or more) features on the outcome is not additive. For example,
“momentum” might only matter on home games: feature A = home/away, feature B = momentum
indicator; neither alone might consistently predict covering the spread, but if momentum is high and the
game is home, the win probability jumps. We need ways to quantify and detect such interactions in the
model.

2

Friedman’s H-Statistic (Interaction Strength): Friedman and Popescu (2008) introduced the H-statistic to
measure the strength of interaction between features based on the model’s behavior 7 8 . The idea is
grounded in the model’s partial dependence functions. Let $\hat f(\mathbf{x})$ be the prediction function
(e.g. predicted log-odds of covering the spread) and consider two features $j$ and $k$. The partial
dependence $PD_{jk}(x_j,x_k)$ is the model’s prediction averaged over all other features when $X_j=x_j$ and
$X_k=x_k$ (it captures the joint effect of $j,k$). Similarly $PD_j(x_j)$ is the partial dependence of feature $j$
alone (averaging out everything else), and $PD_k(x_k)$ for feature $k$. If features $j$ and $k$ do not
interact at all in the model, then the combined effect equals the sum of individual effects:

P Djk (xj , xk ) = P Dj (xj ) + P Dk (xk ) .
This additive decomposition means the model can be written as $\hat f(\mathbf{x}) = g_j(x_j) + g_k(x_k) +
\text{(effects of other features)}$, with no extra term for the pair ${j,k}$. When an interaction is present,
this equality fails – there is some extra “boost” or “penalty” when $j$ and $k$ take certain combinations of
values (like the housing price example where having both a big house and good location adds an extra
premium beyond the sum of individual effects 9 ).
The H-statistic $H_{jk}$ quantifies how large the interaction effect is relative to the total variability of the
two-feature partial function. Mathematically, Friedman defines (for two features $j,k$) 10 :

∑i=1 [ P Djk (xj , xk ) − P Dj (xj ) − P Dk (xk )]
n

2
Hjk
=

(i)

(i)

(i)

∑i=1 [P Djk (xj , xk )]
(i)

n

(i)

(i)

2

2

.

This is essentially the fraction of the variance of the joint partial dependence that cannot be explained by
the sum of the individual partial dependences – in other words, the fraction of variability due to the
interaction between $j$ and $k$. $H_{jk}$ ranges from 0 (no interaction: the numerator is zero because
$PD_{jk} = PD_j+PD_k$ everywhere) to 1 (pure interaction: $PD_j$ and $PD_k$ are flat, and all variation in
$PD_{jk}$ comes from their combination) 11 . In rare cases $H_{jk}$ can exceed 1 if the interaction causes
variation larger than the marginal variation of $PD_{jk}$ itself (an artifact possible if $PD_{jk}$ has low
overall variance but the difference $PD_{jk}-PD_j-PD_k$ has higher variance in sign-swings 12 ). Practically,
$H_{jk}\approx 0$ means features $j$ and $k$ contribute additively (no synergy), while a high $H_{jk}$ close
to 1 indicates a strong interaction (one feature’s effect on the prediction depends heavily on the other
feature’s value). Friedman also defines a total interaction strength for a single feature $j$ with all other
features 13 :

Hj2 =

(i)
(i)
n
∑i=1 [f^(x(i) ) − P Dj (xj ) − P D−j (x−j )]
2
n
∑i=1 [f^(x(i) )]

2

,

where $PD_{-j}$ is the partial dependence on all features except $j$. $H_j$ measures how much feature $j$
participates in any interaction. For instance, if $H_{\text{weather}}$ is high, it means the effect of weather is
mostly through interacting with other factors (perhaps weather amplifies or dampens the impact of other
features like passing efficiency). If $H_j=0$, feature $j$ has only independent additive effects on the
outcome.

3

To derive the $H$-statistic formula intuitively: assume we center all partial dependence functions at zero
mean for consistency 14 . If there are no interactions, we have an additive model and subtracting the sum
of one-way effects would perfectly explain the two-way PD: $PD_{jk} - PD_j - PD_k \equiv 0$. With
interaction, $D_{jk}(x_j,x_k) := PD_{jk} - PD_j - PD_k$ is the pure interaction term (as a function of $x_j,x_k$).
We compute the variance of this term over the joint distribution of $(x_j,x_k)$ (approximated by averaging
over the $n$ sample points). Then we normalize by the variance of the overall two-way partial function
$PD_{jk}$ to get a unitless ratio 15 . Thus $H^2_{jk} = \mathrm{Var}[D_{jk}(X_j,X_k)]/\mathrm{Var}[PD_{jk}
(X_j,X_k)]$. This ratio is $0$ if $D_{jk}$ is identically zero (no interaction), and $1$ if $PD_{jk}$ has no additive
component at all (all variance comes from interaction term). Friedman also proposed a statistical
significance test for $H_{jk}=0$ (null of no interaction) by comparing to a model where that interaction is
artificially removed 16 , but implementing such a test depends on the model class and is complex (beyond
our scope). The H-statistic is a global, model-agnostic measure – it uses the model’s predictions on all data
points, so it summarizes interaction strength across the entire dataset. Computing it can be expensive: to
get $PD_{jk}(x_j,x_k)$ one must average predictions over all other features, which for two features and $n$
data points can require $O(n^2)$ model evaluations in a naïve implementation 17 . In practice one often
samples points or uses faster algorithms, accepting some variance in the estimate 17 .
SHAP Interaction Values: Another way to assess interactions is through SHAP (Shapley Additive
Explanations) values, extended to interaction effects. SHAP values $\phi_i$ decompose a prediction $\hat
f(\mathbf{x})$ into contributions from each feature ($\sum_j \phi_j = \hat f(\mathbf{x}) - \phi_0$, with $
\phi_0$ the average prediction). SHAP interaction values go a step further: they decompose the
contribution of feature pairs. Specifically, one can write the prediction as a sum of main effects $
\phi_i^{(main)}$ and pairwise interaction effects $\phi_{i,j}^{(int)}$:
(main)
(int)
f^(x) = ϕ0 + ∑ ϕi
+ ∑ ϕi,j .
i

i<j

Here $\phi_{i,j}^{(int)} = \phi_{j,i}^{(int)}$ is the portion of prediction attributable to the interaction between
$i$ and $j$. By construction, summing all main and interaction terms recovers the prediction 18 . In SHAP,
these are derived from the concept of the Shapley interaction index in cooperative game theory 19 . The
formula for the interaction value between features $i$ and $j$ is (for $i\neq j$):

ϕi,j =

∑

S⊆F∖{i,j}

∣S∣!(M − ∣S∣ − 2)!
[ f (S ∪ {i, j}) − f (S ∪ {i}) − f (S ∪ {j}) + f (S)] ,
2 (M − 1)!

where $M$ is total number of features, and $f(S)$ denotes the model prediction given only features in
subset $S$ present (others absent) 19 . This equation looks heavy, but conceptually it’s averaging the
marginal interaction effect $\delta_{ij}(S) := f(S\cup{i,j}) - f(S\cup{i}) - f(S\cup{j}) + f(S)$ over all possible
conditioning subsets $S$ of the other features 20 . In other words, $\phi_{i,j}$ is the fair share of the $i$-$j$
interaction contribution to the prediction, accounting for all contexts. If the model had no interaction
between $i$ and $j$, then adding $j$ after $i$ does nothing beyond adding them separately, so $\delta_{ij}
(S)=0$ for all $S$, yielding $\phi_{i,j}=0$. A positive $\phi_{i,j}$ means that together $i$ and $j$ increased the
prediction by that amount beyond their individual contributions (negative would mean their combination
reduces the prediction unexpectedly).

4

Comparison – Interpretation and Usage: Both $H$-statistic and SHAP interaction values aim to quantify
interaction effects, but they differ in scope and scale:
• H-statistic is global: It tells us how much of the model’s overall variance (or prediction function
variability) is due to a particular interaction. For example, if $H_{\text{QB rating, Weather}} = 0.3$,
roughly “30% of the variation in predictions as a function of quarterback rating and weather can be
attributed to their interaction.” This is useful for ranking which interactions are most important
globally. A high $H_j$ for feature $j$ would warn us that $j$ mostly matters in combination with
others (not on its own).
• SHAP interaction values are local (instance-level): For a given game (data point), $\phi_{i,j}(x)$ quantifies
how much the interaction between $i$ and $j$ contributed to that game’s predicted outcome. For
instance, for a particular game, the SHAP interaction of (home-field, momentum) might be +0.05 in
probability terms, indicating the model attributed a +5% boost to the win probability due to the
synergy of being at home and having momentum. By averaging or summing these interaction values
over all instances, we can also get global metrics (e.g. mean $|\phi_{i,j}|$ as the overall importance
of that interaction). In fact, one can show that summing the squared interaction SHAP values over
the data relates to the variance-based interaction strength (since SHAP is a form of ANOVA
decomposition) – qualitatively, if an interaction has large SHAP values across many instances, it will
produce a high $H_{jk}$ as well.
• Relationship between H and SHAP: Both are rooted in the idea of comparing the full model to an
additive (no-interaction) version. The numerator of $H^2_{jk}$ is essentially $\sum_i [f(x^{(i)}) f_{\neg j,k}(x^{(i)})]^2$ where $f_{\neg j,k}$ is the model with that interaction removed. SHAP
interaction $\phi_{i,j}(x)$ is like a distributed version of that difference for each instance (not squared,
but directly in prediction units). If one were to sum $\phi_{i,j}(x)$ over all instances, the positive and
negative interaction contributions might cancel (whereas $H^2$ uses variance, i.e. squared
differences). Thus, $H$ focuses on magnitude of interaction effect in terms of variance fraction,
while SHAP provides a full allocation of interaction effects to each prediction. There isn’t a simple
closed-form link (they are different perspectives), but they are conceptually consistent. For example,
if $\phi_{i,j}$ is large in absolute value for many instances, it means sometimes $i$ and $j$ together
strongly swing predictions – this would manifest as a high $H_{jk}$ because $PD_{jk}$ cannot be
explained additively. If an interaction only matters in a few cases but is zero in most, SHAP will show
large values on those cases (and zero on others), whereas $H$ might show a moderate strength
(since variance considers frequency). In short, $H$-statistic is like a global $R^2$ for the interaction
15 , whereas SHAP interaction values break down the contribution of interactions for each
prediction 18 .
• Practical use: Use Friedman's $H$ to screen for important interactions: e.g., calculate $H_{jk}$ for
all pairs or for each feature $j$ overall. If any $H_{jk}$ or $H_j$ is high, that flags an interaction
worth investigating. In an NFL model, one might find $H_{\text{wind, passing offense}}$ is high,
indicating wind interacts with passing efficiency (perhaps strong winds diminish the impact of a
good passing game disproportionately). Then use SHAP interaction values or partial dependence
plots to interpret how the interaction works – e.g., plot the SHAP dependence of feature $i$ colored
by feature $j$ to see the interaction pattern 18 21 . SHAP interaction values are also readily
computed for tree models (TreeExplainer can output an $M\times M$ matrix of interactions for each

5

instance), enabling us to identify on a per-game basis if a combination of factors contributed to a
win prediction.
In summary, H-statistic provides a mathematical measure of interaction strength (with 0 meaning strictly
additive, and larger values indicating stronger interaction up to 1 or more) 11 , while SHAP interaction values
provide a decomposition of predictions into interactive effects, satisfying game-theoretic fairness. Both
are valuable: $H$ to prove an interaction exists and gauge its importance, and SHAP to explain and visualize
the interaction effect in the context of actual predictions. Mathematically, they are related by the fact that
SHAP interaction values, when appropriately aggregated (summing variances or expectations), connect to
the variance explained by interactions (the basis of $H^2$), but SHAP offers much more granular insight.

3. Dimensionality Reduction Without Losing Predictive Power
High dimensionality (hundreds of features) can harm model performance due to the curse of
dimensionality and overfitting. Reducing dimensionality while retaining the information relevant to
prediction is therefore crucial. We consider three approaches: feature projection methods (like PCA),
representation learning (autoencoders), and feature selection (already discussed). Each has different
use-cases:
• Principal Component Analysis (PCA): PCA is an unsupervised linear dimensionality reduction
technique. It finds orthogonal linear combinations of the original features (principal components)
that successively explain the maximum variance in the data. By projecting the data onto the top $K$
principal components, one can often capture most of the variance in far fewer dimensions. PCA is
effective when many features are correlated or mostly contain redundant information. For example,
if 10 features are various offensive stats that all correlate, their information essentially lies in a
lower-dimensional subspace; PCA might compress them into a couple of components (e.g. one
principal component could capture overall offensive strength). PCA is optimal when variance in
features is a good proxy for predictive signal, and when linear relations structure the data. It is
often used to alleviate multicollinearity: by using principal components instead of original features,
one produces orthogonal features (correlation = 0 by construction) which can stabilize linear model
training. For instance, a logistic regression might benefit from PCA if many inputs are collinear.
However, PCA does not use the target variable – it ignores which directions are most predictive in
favor of those that have most variance. If a predictive feature has low variance (e.g. a rare but
important event feature), PCA might discard it. In sports betting data, some crucial signals might be
subtle (small variance) – PCA could inadvertently drop the “signal” dimensions while preserving
“noise” dimensions that have high variance but are irrelevant to outcome. Thus, PCA is best when we
believe that most variance is informative, or as a preprocessing step to combat high dimensional
noise.
• Autoencoders: An autoencoder is a type of neural network that learns an encoding (compression)
of the input data into a lower-dimensional latent space and can decode it back to approximately the
original. In essence, it’s a non-linear generalization of PCA. An autoencoder can learn complex nonlinear combinations of features in the code (latent variables). Use autoencoders when feature
relationships are highly non-linear or when you suspect the data lies on a non-linear manifold. For
example, an autoencoder could potentially compress categorical combinations or interactions in a
way PCA (linear) cannot. Autoencoders can thus achieve better compression with less loss of
information on complex data. Caveats: (1) Autoencoders require substantial data to train (they have

6

many parameters); with only ~2,500 samples, a deep autoencoder could easily overfit, essentially
just memorizing the inputs rather than finding a meaningful low-dimensional structure. (2) Like PCA,
a standard autoencoder is unsupervised – it compresses to preserve as much information about $X$
as possible (minimizing reconstruction error), not specifically to preserve information about $Y$.
One can incorporate $Y$ (through a supervised loss or an architecture like a “predictive
autoencoder”), but that becomes more of a specialized model. Given our scenario (tabular data,
moderate size, emphasis on interpretability), autoencoders are less commonly used. If the dataset
were much larger or the features very complexly related, a variational autoencoder or nonlinear
embedding might find a lower dimensional representation capturing the essence of team
performance, etc. But lacking data, autoencoders might lose predictive power by overcompressing or overfitting.
• Feature Selection: Unlike PCA or autoencoders, feature selection keeps a subset of original features
and discards the rest, rather than creating new composite features. This is often preferable for
interpretability – especially in sports analytics, we want to say “these 20 stats matter the most”
rather than using obscure combinations. From a predictive standpoint, if indeed only a small fraction
of features carry most of the information about the outcome, selecting those will reduce model
complexity (fewer inputs) without sacrificing accuracy. Feature selection can be seen as a form of
dimensionality reduction that results in sparse models (e.g. using L1 regularization or tree-based
selection). The risk is that if important information is spread across many features or in
combinations, naive feature selection might drop individually weak but jointly strong features.
However, methods like mRMR (maximum relevance, minimum redundancy) explicitly try to
choose features that together capture the target’s information (using mutual information criteria)
22 . In practice, given our goal of 20–30 features out of 500, feature selection (with careful methods
to account for interactions or using a powerful model in the loop) is likely the chosen route – it yields
a simpler model that’s easier to validate and less prone to overfit.
When to use which: If you find that many features are highly correlated and essentially measure the same
underlying factor, and you do not need to interpret each original feature, PCA is a quick way to collapse
those into one factor. For example, instead of three collinear weather metrics, the first principal component
might capture “overall weather severity” which could be used as a single feature. PCA is also useful as a
preprocessing step if you plan to use distance-based methods or need to decorrelate features.
Autoencoders are useful if you suspect non-linear redundancy – e.g. perhaps the data has clusters or
curved manifolds (like perhaps team performance metrics lie on a non-linear curve of offensive vs defensive
tradeoffs). Autoencoders could reduce dimensionality more effectively in such cases, but require more
effort to train and tune. If interpretability and theoretical grounding are important (as indicated by the
user), autoencoders (being black-box transformations) are less attractive here. Feature selection is ideal
when you believe many features are irrelevant or noisy and a handful contain the signal. It directly
addresses the goal of finding “20-30 maximally predictive features” and maintains their original meaning.
Also, since our predictive models (gradient boosting, logistic regression) can handle raw features without
needing them to be orthogonal, it’s often fine to just feed the selected features into the model. In fact, treebased models are robust to multicollinearity (they can split on one of a set of correlated features without
issue, though the presence of many correlated features can dilute individual importances).
Information-Theoretic Perspective: Any dimensionality reduction is effectively a mapping $Z = g(X)$ to a
compressed representation $Z$ of lower dimension $d < D$. A crucial theorem is the Data Processing
Inequality, which states that no deterministic mapping can increase the mutual information with the

7

target: $I(Z; Y) \le I(X; Y)$ 23 . In other words, compression can only throw away information, not add.
The best scenario is $I(Z;Y) = I(X;Y)$, meaning $Z$ retains all predictive information present in $X$ (i.e. $Z$ is
a sufficient statistic for $Y$). If $g$ is invertible (lossless compression), then $I(Z;Y)=I(X;Y)$, but true
dimensionality reduction is lossy. The challenge is to minimize the loss of relevant information. The
Information Bottleneck principle formalizes this: we seek a compressed representation $Z$ that maximizes
$I(Z;Y)$ while minimizing $I(Z;X)$ (compression cost). One can derive bounds on the trade-off: for a given
level of compression (say $I(Z;X)$ bits), there’s an upper bound on achievable $I(Z;Y)$ known as the
information bottleneck curve 24 . Without diving into derivations, an intuitive bound is that to predict $Y$
accurately, $Z$ must contain nearly all the entropy of $Y$ that was present in $X$. If $Y$ is binary with
entropy ~$H(Y)$ (in our case $H(Y)\approx 1$ bit for a balanced win/loss against spread), then $I(X;Y) \le H(Y)
\approx 1$ bit. If the features are good, maybe $I(X;Y)$ is, say, 0.1 bits (which would correspond to some
improvement over random guessing). If we compress $X$ to too few dimensions, we might reduce $I(Z;Y)$
below what’s needed for 52.4% accuracy. For example, achieving 52.4% accuracy (2.4% better than random)
corresponds to an extremely small mutual information (on the order of $10^{-3}$ bits)【27†】 – not much
needs to be lost to forfeit that edge. Thus, the information-theoretic bound for “no loss in predictive
power” is that the compression must preserve the entire mutual information $I(X;Y)$. In practice, this
means the chosen features or components must capture all systematic patterns in $X$ that relate to $Y$. If
some information is orthogonal to the top principal components (for PCA) or not learned by the
autoencoder, it will manifest as a drop in model accuracy.
Another perspective: Fano’s inequality gives a lower bound on classification error in terms of conditional
entropy. It implies that if $I(X;Y)$ is small, any classifier will have error close to 50%. Conversely, to achieve a
low error (far above 50% accuracy), $I(X;Y)$ must be large. In our betting context, the required 52.4%
accuracy is just barely above chance, so even a tiny loss of information could drop us back to <52.4%. This
suggests using dimensionality reduction with caution – prefer methods that explicitly keep predictive info.
Feature selection using mutual information or model-based importance tries to do exactly that: keep
features with high $I(X_j;Y)$ or contributions to $I(X;Y)$. Techniques like mRMR seek a subset $S$
maximizing $I(S;Y)$ while minimizing redundancy $I(S;X\setminus S)$ 22 . On the other hand, PCA
guarantees preservation of total variance, but not of $I(X;Y)$ (unless target correlation aligns with top
components).
Information-Theoretic Compression Limits: If features were truly compressible without loss of prediction,
it means there is an underlying low-dimensional structure to the data with respect to $Y$. For example, in
binary classification problems, often the effective dimensionality is at most $1$ – specifically, the Bayesoptimal classifier uses the log-odds $\log \frac{P(Y=1|X)}{P(Y=0|X)}$ as a single-dimensional sufficient
statistic. All features influence the outcome only through this single-number (the probability of covering). If
we somehow knew that function, we could reduce $X$ to a 1-D value (the predicted probability) and not lose
any accuracy. Of course, we don’t know it in advance – learning that function is equivalent to solving the
classification problem. But this tells us that in principle, we could compress to 1 dimension without losing
predictive power if we found the right mapping. Methods like Linear Discriminant Analysis (LDA) leverage
this idea: for binary $Y$, LDA finds the single linear combination of features that maximizes class separation
(assuming Gaussian class distributions). LDA yields a 1D score that is the sufficient statistic under its model
assumptions. In general, if the relationship is highly non-linear, you might need more dimensions to form a
sufficient representation.
From a coding theory viewpoint, if we compress features into $d$ real-valued dimensions, how many bits
of information can that carry about $Y$? Potentially infinite, if the real values are continuous and high-

8

precision. But with finite data, effectively there’s a limit. One can think in terms of rate-distortion theory: to
achieve a certain “distortion” (classification error) you need a certain rate (information in bits). For near-zero
distortion (very high accuracy), you need to preserve most of $H(Y|X)$ information. For our small edge,
even a slight compression can cause a noticeable drop in accuracy.
Conclusion of this section: Use PCA when you need to quickly reduce dimensionality and multicollinearity
in an unsupervised way (e.g. to avoid overfitting in a very high-D setting or to visualize data), keeping in
mind you might lose some predictive signal if it’s not aligned with major variance components. Use
autoencoders if linear methods are insufficient and you have enough data to learn non-linear feature
manifolds – they can achieve greater compression by capturing complex structure, but risk overfitting on
small data and are harder to interpret. Use feature selection when you aim to keep interpretability and
believe that only a subset of features contribute to prediction. Feature selection directly tackles the problem
of information compression with respect to $Y$ by discarding irrelevant features; theoretically, it seeks to
retain as much $I(X;Y)$ as possible in the subset. Information-theoretic bounds remind us that we cannot
compress below the intrinsic information content needed for prediction without sacrificing accuracy. In
practice, a safe approach is to compress only to the point where validation performance starts to degrade.
If 20 features achieve the same accuracy as 500, it implies those 20 captured essentially all the information
needed (the remaining features were redundant or noise). If compressing further to 5 features drops
accuracy, it means we’ve passed the information limit – those 5 don’t contain enough bits of $Y$. Thus,
theory guides us to seek the minimal sufficient subset, but warns that too aggressive compression will
increase prediction error (as seen via Fano’s inequality, error must rise if information falls too much 25 ).

4. Multiple Testing Corrections in Feature Selection
With 500+ candidate features, the danger of false discoveries (spurious correlations that appear significant
by chance) is high. If we independently test each feature for association with the target (e.g. a t-test for
difference in win rate, or a correlation test), at the conventional $p<0.05$ significance level we expect about
$0.05 \times 500 = 25$ false positives on average even if none of the features truly has predictive power.
This is the multiple comparisons problem. We must adjust our confidence thresholds to control for this.
Bonferroni Correction (Family-Wise Error Rate): The Bonferroni method is a simple and conservative way
to control the family-wise error rate (FWER) – the probability of making at least one Type I error (false
positive) among all tests. It says to test each hypothesis at level $\alpha/m$ if we want an overall $\alpha$
level for $m$ tests. For $m=500$ and desired 95% overall confidence (α = 0.05 FWER), the required perfeature p-value threshold would be $0.05/500 = 0.0001$ 26 . In other words, a feature’s association must
have $p < 1\times10^{-4}$ to be deemed significant after Bonferroni correction. This is extremely stringent
– it greatly reduces false positives (approximately $5\%$ chance of even one false feature being selected)
but at the risk of false negatives (missing true signals that aren’t that strong). If an underlying feature truly
has a modest effect (say $p \sim 0.001$ under null), Bonferroni would fail to flag it because 0.001 > 0.0001.
In sports betting, where signals can be weak (the “edge” might be small), Bonferroni correction may be too
harsh, potentially dismissing real edges as not significant. However, if we were to go on to bet money based
on identified features, a strict control might be warranted – a false discovery could cause us to wrongly
identify a betting angle and lose money. Bonferroni gives strong control of the “at least one false lead”
probability. With 500 features, using $\alpha=0.0001$ ensures only a 5% chance that any feature is
erroneously flagged as predictive due to random chance.

9

False Discovery Rate (FDR) – Benjamini-Hochberg (BH) Procedure: FDR control offers a less stringent
criterion, allowing some proportion of discoveries to be false, in exchange for higher power (ability to detect
true effects) 27 . The FDR is the expected fraction of selected features that are false positives 28 . For
example, controlling FDR at 5% means: on average, 95% of the features we declare “significant” will truly be
predictive, and 5% may be bogus. This is often acceptable in exploratory research or situations like ours
where we might prefer to cast a slightly wider net and then further validate the findings (e.g. on a test set or
next season’s data). The Benjamini-Hochberg algorithm sorts the $p$-values $p_{(1)} \le p_{(2)} \le \dots
\le p_{(m)}$. It finds the largest index $k$ such that $p_{(k)} \le \frac{k}{m}\alpha$ 29 . Then it declares
features $1,2,\dots,k$ significant (and those have an FDR $\le \alpha$). For $\alpha=0.05$ and $m=500$, the
threshold for the $k$-th smallest p-value is $\frac{k}{500}\times 0.05$.
Let’s do a short calculation to illustrate: If we aimed for (approximately) 95% confidence that a discovery is
not false (i.e. FDR 5%), and suppose the procedure selected $k=20$ features. The 20th smallest p-value
would need to satisfy $p_{(20)} \le \frac{20}{500}\cdot0.05 = 0.002$. So roughly, to get 20 features, the 20th
best feature needs $p\approx0.002$ or better. If only a handful of features have such small p-values, we’ll
select only that handful. If none have $p<0.0001$, BH might still select some if there are many with
moderately small p: for instance, if 25 features all have $p\approx 0.001$, then $p_{(25)}=0.001$ which is $
\le 25/5000.05 = 0.0025$, so BH would include them. In contrast, Bonferroni would include none of those since
0.001 > 0.0001. Thus BH is more powerful* when there are multiple signals, accepting a controlled fraction of
false positives 27 . In the context of sports betting, using FDR could mean we allow a few features that are
actually noise to slip through, but we gain the ability to detect more true features that have mild effects. We
must then manage those false positives via further validation (for example, verify that any selected feature
still performs well on the held-out test set – a spurious feature would likely not maintain its predictive
power out-of-sample).
Correlated Tests: A wrinkle is that both Bonferroni and the standard BH procedure assume tests are
independent or at least positive regression dependent (a certain type of positive correlation) 30 31 . In
feature selection, test statistics will often be correlated because features are correlated. For example, two
highly correlated features might both show “almost significant” p-values, but the chance of both being
extreme under null isn’t independent. Bonferroni remains valid under any dependence (it’s just a union
bound), albeit conservative. BH is valid under certain dependence conditions (PRDS – positive regression
dependent on subsets). If the dependencies are arbitrary, BH can have slightly higher FDR than nominal.
There is a refinement known as Benjamini–Yekutieli (BY) that adjusts the threshold to $\frac{\alpha}{m
\cdot \sum_{i=1}^m 1/i}$ 26 , which for large $m$ is roughly $\alpha/(m \ln m)$. For $m=500$, $\sum_{i=1}
^{500}1/i \approx 6.79$, so BY would use $\alpha’ = 0.05/(5006.79) \approx 1.47\times10^{-5}$, even more
strict than Bonferroni in this case. BY controls FDR for any dependency structure but at a heavy cost in power 32 .
In practice, many use BH even with correlated tests and find it works reasonably, especially if the correlations
mean there are clusters of features capturing the same signal (so we might get several low p-values for one true
effect – BH will flag several, possibly tolerating that one or two might be null among them). Another approach is to
reduce the number of tests by grouping correlated features* (e.g. test an entire cluster of highly correlated
features as one composite). But this enters domain-driven decisions.
Required p-values for 500 features at 95% confidence: Interpreting this as controlling the overall false
positive risk at 5%, the Bonferroni answer is p < 0.0001 for each feature 26 . If interpreting it as controlling
FDR at 5%, then on average any feature with $p \ll 0.05$ might be selected depending on the ranks. For
instance, if we only select features with $p < 0.001$, then even if we select a few, the expected proportion of
false ones will be low (this is more stringent than needed for FDR 5% if only a few are selected). In a

10

nutshell: to maintain high confidence in discoveries with 500 comparisons, adjust your p-value threshold
downward by roughly a factor of 500. Bonferroni’s linear scaling is the simplest: $\tilde{\alpha} = 0.05/500
\approx 1\times10^{-4}$. Under FDR, the effective threshold depends on how many signals there are. If
there are many true signals, BH will let in some with p slightly above $1\times10^{-4}$. If there are none, BH
will select none anyway (so it controls false leads similarly).
Data Mining Bias and Overfitting: It’s worth noting that multiple testing correction addresses the explicit
statistical tests (like filtering features by p-value). In feature discovery, one might also implicitly overfit by
trying many modeling combinations, looking at test results repeatedly, etc. This can introduce a bias where
you overestimate performance (because with many trials, one will luck out on test set too). That’s beyond
classic multiple testing but important. Techniques like hold-out validation and cross-validation with
proper discipline (not peeking at test set until final) are essential to ensure that after feature selection, the
performance is real. In context, if we try 500 features or combinations, the chance that one yields ~53%
accuracy on train by luck is high. Bonferroni gives a rough bound: if we test 500 features for any predictive
ability, the probability that one achieves a spurious high accuracy significant at 0.05 is $\le 0.05$ 26 (FWER
control). Without correction, the “data mining bias” practically guarantees some feature will seem to beat
52.4% on training just by randomness. Thus, one should either use corrections or evaluate significance by
permutation tests (e.g. shuffle game outcomes many times and see if any feature in random data appears
as predictive as in the real data – this provides a empirical null distribution for the best feature
performance).
In summary, for 500 features: a Bonferroni-corrected 95% confidence requires $p < 0.0001$ per feature to
be considered truly significant 26 . Controlling FDR at 5% would allow a less strict cutoff (approximately on
the order of $10^{-3}$ if multiple features show signals). In a sports betting context, using FDR might be
more practical to not miss potentially useful factors (we accept maybe 1 false factor out of 20) 27 . But any
features discovered still need out-of-sample verification. Indeed, one might combine approaches: use a
lenient selection (maybe pick the top 30 features by some criterion), then validate those on the 30% test set.
The test set performance will reveal which ones were likely spurious (their effects won’t replicate). This way,
we avoid both overfitting and missing real signals.

5. Causal vs Correlational Features: Identifying Spurious Signals
One of the hardest parts of sports modeling is that correlation does not imply causation. Features can be
correlated with covering the spread for incidental or confounding reasons rather than truly causing better
performance. We want features that capture genuine causes or at least stable predictive patterns, rather
than flukes or artifacts of the data.
Spurious Correlations: A spurious correlation is when a feature $X$ and outcome $Y$ appear related in the
sample but the relationship is not causal or stable – it might be due to chance, or both might be driven by a
third factor. In NFL data, for example, “winning percentage in night games” might correlate with covering
the spread, but that could be because good teams both win at night and cover spreads; the time of day
itself isn’t causing better play. Methods to detect spurious correlations include:
• Hold-out validation and consistency: Check if the feature’s predictive power holds on new data. A
spurious feature often fails to predict future outcomes even if it fit past data well. If in your training
set feature A had a 60% cover rate vs 40% otherwise ($p<0.01$), but in the test set it’s 50% vs 50%,
that indicates it was likely spurious. Consistency over time (e.g. season by season) is a good sanity

11

check – true causal factors tend to persist (unless the game fundamentally changes), whereas
random correlations flicker.
• Control for Confounders: Use multiple regression or partial correlation to see if the feature still has
effect when known confounding variables are included. For example, if you suspect a feature is
actually proxying for team skill, include team strength ratings in the model; if the feature’s
importance vanishes, it wasn’t fundamental. This is essentially building a causal graph: if $X$
correlates with $Y$ only because both are caused by $Z$, then conditioning on $Z$ (or including $Z$
in the model) will nullify $X$’s effect. For instance, “divisional game” might correlate with covering
(maybe underdogs in divisional games cover more often). But if you include the Vegas spread or
team familiarity metrics, that effect might disappear – indicating divisional status itself isn’t causal,
it’s just that spreads are set differently or teams approach those games differently (already
accounted for by other features).
• Domain knowledge and plausibility: Ask if there is a plausible mechanism for the feature to affect
the outcome. If not, be wary. Features like EPA/play have an obvious causal link to winning (move the
ball efficiently -> score more -> cover spreads). A feature like “uniform color” does not (any
correlation is likely coincidence or linked to a team identity). Using subject matter expertise can filter
out many spurious candidates before even testing.
• Permutation tests for significance: As touched on above, one robust way to assess if a feature’s
performance is real is to use a target permutation test. If we randomly shuffle the outcome labels
(cover/not) across games and re-evaluate the feature’s association many times, we get a null
distribution of how often a feature would seem predictive by luck 33 . If the actual feature’s
performance (e.g. accuracy or correlation) is in the extreme tail of that null distribution, then it’s
likely a true signal; if not, it might be spurious. This method inherently accounts for multiple testing
as well – by seeing if our best-found feature is stronger than what “best features” in random data
would be. For example, if in random data the best out of 500 features often gets to 55% accuracy by
chance, then a feature that achieved 54% in real data is not impressive.
• Causal inference techniques: In some cases, we might bring in formal causal inference (though it’s
hard without experimental data). If we have some temporal ordering, we could use Granger
causality for time series features (does past feature values help predict future outcome beyond
what we already know?). For instance, a “momentum” feature derived from previous games could be
tested if it genuinely adds predictive power beyond team strength – if not, its correlation might be
spurious or just reflecting team quality streaks. Instrumental variable approaches or causal graphs
are advanced tools that typically require more detailed data and assumptions.
Permutation Importance vs. SHAP for inferring causality:
• Permutation Feature Importance: This is a model-agnostic technique: after training the model, we
randomly shuffle (permute) the values of a particular feature in the validation or test set and see
how much the model’s performance degrades. If shuffling feature $X$ breaks the model’s accuracy
significantly, it suggests $X$ was contributing unique predictive information – in other words, the
model relied on $X$ in a meaningful way 34 35 . If shuffling $X$ has little to no effect on
performance, then either $X$ was not important or whatever information it had was redundant with
other features (the model could do without it). Permutation importance is especially useful to detect

12

spurious or redundant features. For example, suppose the model gave some weight to feature $X$
because in training data it correlated with the target, but in truth that correlation was coincidental.
On new data, $X$ will not actually help predict, so the model’s reliance on it is effectively harmful or
useless. When we shuffle $X$, the model’s performance might even improve slightly or remain
unchanged – indicating $X$ wasn’t truly predictive (a strong sign of spurious correlation or
overfitting). In fact, permutation importance can be negative: if the model has overfit to noise in
$X$, randomizing $X$ removes that noise influence and can improve generalization 35 . Such a
scenario screams that $X$ was spurious – the model was better off not using it. In our context, if a
feature has high training SHAP importance but zero permutation importance on validation, it’s likely
a red flag (the model latched onto an unreliable pattern with that feature).
• SHAP Values: SHAP (non-interaction) values $\phi_i$ explain how each feature contributes to a
specific prediction compared to the average prediction. They are excellent for understanding how the
model is using a feature on a case-by-case basis and overall (by summary plots of $\phi_i$). However,
SHAP values are agnostic to causality – they explain correlations learned by the model. If a feature
is spurious and the model erroneously uses it, SHAP will faithfully report that the feature contributed
to predictions (perhaps even with large $\phi$ values). SHAP does not tell you if the model should be
using that feature, only that it does. Additionally, SHAP values can be misleading under feature
correlation 36 . If features are correlated, the credit assignment can be ambiguous: SHAP might
split credit among correlated features in a somewhat arbitrary way (depending on the orderings in
the Shapley coalition integrals), sometimes even giving opposite signed contributions or
exaggerating one feature’s effect when two move together 36 . For example, if temperature and
humidity are highly correlated, and only their combination matters (but not individually), SHAP might
still assign each some importance. Or if one of them is redundant, SHAP might distribute importance
evenly, making both look moderately important, when actually either alone would suffice. In
extreme cases, as reported by Mase et al. (2021) and others, assuming feature independence (as
Kernel SHAP does by default) can lead SHAP to invent importance for a feature that had no causal
influence, purely because in unrealistic data permutations that feature can drive predictions 37 .
In terms of causality: Permutation importance is closer to testing causality (in the sense of does the
model’s prediction ability drop if this feature’s information is destroyed? – analogous to a do-intervention
$do(X=\text{random})$). If the model’s accuracy significantly worsens, it implies $X$ had unique predictive
content that wasn’t explainable by other features – a hint that $X$ might be a causal driver or at least a
necessary predictor. If accuracy doesn’t change, $X$ was not needed – either redundant or spurious. This
aligns with the idea of causality: a truly causal feature (or a necessary proxy for a cause) should affect
outcomes when perturbed (here, outcomes as predicted by the model). A non-causal correlation (one the
model learned by coincidence) often won’t generalize, so permuting $X$ (especially on an independent test
set) won’t hurt much because the model’s reliance on $X$ was unwarranted.
Practical approach: We can use permutation importance to vet our selected features. After training the
model on training data with the chosen features, we evaluate on test data: permute each feature and
measure the drop in, say, AUC or accuracy. If a feature causes little drop, we question its utility. If it causes a
big drop, it’s likely a true predictor (though not proof of causality in a philosophical sense, it is at least
predictively causal for the model). For example, if “EPA/play” when permuted reduces accuracy notably, it’s a
solid feature; if “coin toss winner” when permuted does nothing, then even if the model had some weight
on it, it was spurious.

13

SHAP values, on the other hand, are great for inspecting model behavior and finding patterns (like “when
this feature is high, it usually adds +X to win probability”). They can occasionally highlight weird usage: e.g.
if a feature has a big positive SHAP value in some instances and big negative in others without a logical
pattern, it might indicate the model found some spurious interaction or effect (perhaps overfitting to noise).
But one must be careful interpreting SHAP in correlated settings – correlated spurious features can still get
moderate SHAP contributions due to how credit is allocated 36 . One mitigation is using conditional SHAP
or grouped features, but that’s advanced.
Identifying spurious correlations beyond model importance: Another method is stability selection –
vary your training sample or use bootstrapping to see if the same features keep getting selected. Spurious
ones might pop in one subset and disappear in another. If a feature is truly related to outcome, it should be
selected consistently across different samples (with high probability).
Finally, causality vs correlation is a deep topic; ultimately, a betting model doesn’t necessarily need true
causal features (it just needs stable correlations). However, truly causal features (like a metric that
fundamentally measures team quality) are more likely to be stable predictors going forward, whereas
purely correlational ones might vanish once conditions change. For example, “home underdogs after a loss
cover X%” could have been true for a decade (correlation), but unless there’s a causal reason (maybe
bookmakers systematically misprice that scenario), it could stop working; whereas a causally relevant
feature like “turnover margin” will likely always matter in games.
Summary: Use permutation importance on a validation/test set as a heuristic for causality – if
shuffling a feature obliterates predictive performance, that feature carries unique signal (likely not
spurious) 35 . Use SHAP to understand model decisions, but be wary that it reflects the model’s learned
correlations, which could include spurious ones. If features are highly correlated, note that SHAP might
misattribute importance or split it; in such cases, consider grouping those features or using domain
reasoning to decide which is the actual driver. When possible, back up inclusion of a feature with real-world
reasoning (does it make football sense?). And always validate on fresh data – a truly predictive (whether
causal or just stable correlation) feature will continue to give an edge on new data, whereas a spurious one
will fail. In the end, our feature discovery should prioritize features that not only had statistical evidence in
sample (with corrections for multiple tests), but also pass these sniff tests for being genuine signals rather
than noise or coincidences.
References:
1. Álvaro V. “Correlation vs Mutual Information vs let-the-model-decide.” Data Science Stack Exchange
(2022) – Experiment showing correlation catches linear relationships, mutual information catches
non-linear (quadratic) cases 1 .
2. Arun Aniyan. Answer on Data Science Stack Exchange: Recommending to remove highly correlated
features first, then apply advanced feature selection like mutual information, noting no single
method always wins 4 38 .
3. Neptune.ai Blog. “Feature Selection Methods and How to Choose Them.” (2021) – Discussion of filter vs
wrapper methods; notes that mutual information is non-parametric and needs many samples,
Pearson correlation only catches linear relations 2 .

14

4. Friedman, J.H. & Popescu, B. “Predictive Learning via Rule Ensembles.” Annals of Applied Statistics 2(3):
916–954 (2008) – Introduced H-statistic for interaction strength 7 10 .
5. Molnar, C. “Interpretable Machine Learning – Chapter 21: Feature Interaction.” (2022) – Explanation of
Friedman’s H-statistic with formulas and examples 11 10 .
6. Molnar, C. “Interpretable Machine Learning – Chapter 18: SHAP.” (2022) – Defines SHAP interaction
values and their computation via Shapley interaction index 19 .
7. Benjamini, Y. & Hochberg, Y. “Controlling the False Discovery Rate: A Practical and Powerful Approach to
Multiple Testing.” J. Royal Stat. Soc. B 57(1):289–300 (1995) – FDR procedure that is less conservative
than Bonferroni, allowing more discoveries on high-dimensional data 27 .
8. Stats StackExchange – “FDR correction when tests are correlated.” – Notes that BH procedure assumes
independent or positively dependent tests; in case of correlations, one might use the Benjamini–
Yekutieli adjustment for safety 30 26 .
9. Medium (K. Shankaran). “Interpreting ML Models: Permutation Importance.” (2020) – Emphasizes that
permutation importance can uncover spurious correlations and data leakage by seeing little change
or improvements when permuting a supposedly important feature 35 .
10. Cross Validated (stats.SE) – “Are SHAP values misleading with correlated predictors?” – Confirms that
with correlated features, SHAP values can be misleading (imprecise or opposite sign) due to the
independence assumption 36 . It references that when correlation is moderate (~0.2) or higher,
naive SHAP can deviate from true attribution.

machine learning - Correlation vs Mutual Information vs let-the-model-decide - Data
Science Stack Exchange
1

4

5

6

22

38

https://datascience.stackexchange.com/questions/115225/correlation-vs-mutual-information-vs-let-the-model-decide
2

3

Feature Selection Methods and How to Choose Them

https://neptune.ai/blog/feature-selection-methods
7

8

9

10

11

12

13

14

15

16

17

21 Feature Interaction – Interpretable Machine Learning

https://christophm.github.io/interpretable-ml-book/interaction.html
18

19

20

21

18 SHAP – Interpretable Machine Learning

https://christophm.github.io/interpretable-ml-book/shap.html
23

25

33

Information Theoretic Methods for Variable Selection—A Review

https://www.mdpi.com/1099-4300/24/8/1079
24

[D] understanding the "bottleneck" principle in machine learning

https://www.reddit.com/r/MachineLearning/comments/n06bp2/d_understanding_the_bottleneck_principle_in/
26

30

31

false discovery rate - FDR correction when tests are correlated - Cross Validated

https://stats.stackexchange.com/questions/205516/fdr-correction-when-tests-are-correlated

15

27

28

False discovery rate - Wikipedia

https://en.wikipedia.org/wiki/False_discovery_rate
29

32

The Benjamini-Hochberg procedure (FDR) and P-Value Adjusted Explained | R-bloggers

https://www.r-bloggers.com/2023/07/the-benjamini-hochberg-procedure-fdr-and-p-value-adjusted-explained/
34

Feature Importance for Any Model using Permutation - Medium

https://medium.com/@T_Jen/feature-importance-for-any-model-using-permutation-7997b7287aa
35

Interpreting ML Models: How Permutation Feature Importance Builds ...

https://medium.com/@connect.hashblock/interpreting-ml-models-how-permutation-feature-importance-builds-realtrust-8f663a643ffa

regression - Are SHAP values potentially misleading when predictors are highly correlated? - Cross
Validated
36

37

https://stats.stackexchange.com/questions/535624/are-shap-values-potentially-misleading-when-predictors-are-highly-correlated

16

